{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils_atilla as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "x, y = data['x'], data['y']\n",
    "x = np.concatenate([np.ones((x.shape[0], 1)), x], axis=1)\n",
    "t = np.arange(-1, 1, 0.01).reshape((-1, 1))\n",
    "t = np.concatenate([np.ones((t.shape[0], 1)), t], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_small, y_small = x[:15], y[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex1 Build a scatter plot for x_small, y_small. You may want to look at plt.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_small[:, 1], y_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex2 Fit a simple linear regression with lr=0.05 and plot the evolution of losses. You may want to look at utils file and at plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "opt = utils.GD(0.05)\n",
    "lr = utils.LR(num_features=2, optimizer=opt)\n",
    "losses = lr.fit(x_small, y_small)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex3 Calculate model predictions over the values of t and plot them together with the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_small[:, 1], y_small, color='blue')\n",
    "plt.plot(t[:, 1], y_pred, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex4 Define a function which takes a matrix x (first column is constant 1), int deg and returns matrix x_poly which has first column as constant 1 and other columns\n",
    "are initial columns to the powers k, k=1..deg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_poly(x, deg):\n",
    "    x_poly = x[:, 1:]\n",
    "    #polys = []\n",
    "    for k in range(2, deg+1):\n",
    "        x_poly = np.concatenate([x_poly, x[:, 1:] ** k], axis=1)\n",
    "    \n",
    "    x_poly = np.concatenate([np.ones((x.shape[0], 1)), x_poly], axis=1)\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def make_funcs(x, funcs):\n",
    "    x_poly = x[:, 1:]\n",
    "    #polys = []\n",
    "    for f in funcs:\n",
    "        x_poly = np.concatenate([x_poly, f(x[:, 1:])], axis=1)\n",
    "    \n",
    "    x_poly = np.concatenate([np.ones((x.shape[0], 1)), x_poly], axis=1)\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "x_test = np.array([\n",
    "    [1, 2, 3],\n",
    "    [1, 4, 5]])\n",
    "y_res = np.array([[  1.,   2.,   3.,   4.,   9.,   8.,  27.],\n",
    "                  [  1.,   4.,   5.,  16.,  25.,  64., 125.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "make_poly(x_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(make_poly(x_test, 3), y_res), print('Something is wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex5 Build polynomial regressions for all degrees from 1 to 25 and store their losses. For this exercise use fit_closed_form method instead of GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lrs = {'models': [], 'losses': []}\n",
    "for k in range(1, 26):\n",
    "    x_poly_k = make_poly(x_small, k)\n",
    "    lrs['models'].append(utils.LR(num_features=x_poly_k.shape[1]))\n",
    "    loss = lrs['models'][-1].fit_closed_form(x_poly_k, y_small)\n",
    "    lrs['losses'].append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(list(range(1, 26)), lrs['losses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex6 plot the predicted values over t and scatter of true points for some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for k in range(1, 26):\n",
    "\n",
    "    t_poly = make_poly(t, k)\n",
    "    lr = lrs['models'][k-1]\n",
    "    y_pred = lr.predict(t_poly)\n",
    "    plt.subplot(5, 5, k)\n",
    "    plt.scatter(x_small[:, 1], y_small, color='blue')\n",
    "    plt.plot(t[:, 1], y_pred, color='red')\n",
    "    plt.ylim((-0.15, 0.15))\n",
    "    plt.title(f'deg={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit/Underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex7 Modify the regression's fit method to also get some validation data and output losses over validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LR_valid:\n",
    "    def __init__(self, num_features=1, optimizer=utils.GD(0.1)):\n",
    "        self.W = np.zeros((num_features, 1))\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = X @ self.W\n",
    "        return y\n",
    "    \n",
    "    def one_step_opt(self, X, y_true):\n",
    "        grads = - X.T @ (y_true - X @ self.W) / X.shape[0]\n",
    "        self.W = self.optimizer.apply_grads(self.W, grads)\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss, grads\n",
    "    \n",
    "    def fit(self, X, y_true, X_valid=None, y_valid=None, grad_tol=0.0001, n_iters=1000):\n",
    "        grad_norm = np.inf\n",
    "        n_iter = 0\n",
    "        losses = []\n",
    "        valid_losses = []\n",
    "        while (grad_norm > grad_tol) and (n_iter < n_iters):\n",
    "            loss, grads = self.one_step_opt(X, y_true)\n",
    "            grad_norm = np.linalg.norm(grads)\n",
    "            n_iter += 1\n",
    "            losses.append(loss[0][0])\n",
    "            valid_loss = (y_valid - self.predict(X_valid)).T @ (y_valid - self.predict(X_valid)) / X_valid.shape[0]\n",
    "            valid_losses.append(valid_loss[0][0])\n",
    "        return losses, valid_losses\n",
    "    \n",
    "    def fit_closed_form(self, X, y_true, X_valid=None, y_valid=None):\n",
    "        self.W = np.linalg.inv(X.T @ X) @ X.T @ y_true\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "x_poly = make_poly(x_small, k)\n",
    "x_valid = make_poly(x[100:1100], k)\n",
    "y_valid = y[100:1100]\n",
    "lr = LR_valid(num_features=x_poly.shape[1])\n",
    "losses, valid_losses = lr.fit(x_poly, y_small, X_valid=x_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, color='blue')\n",
    "plt.plot(valid_losses, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for k in range(1, 26):\n",
    "    \n",
    "    x_poly = make_poly(x_small, k)\n",
    "    x_valid = make_poly(x[100:1100], k)\n",
    "    y_valid = y[100:1100]\n",
    "    lr = LR_valid(num_features=x_poly.shape[1])\n",
    "    losses, valid_losses = lr.fit(x_poly, y_small, X_valid=x_valid, y_valid=y_valid)\n",
    "    plt.subplot(5, 5, k)\n",
    "    plt.plot(losses, color='blue')\n",
    "    plt.plot(valid_losses, color='orange')\n",
    "    plt.ylim((0, 0.005))\n",
    "    plt.title(f'deg={k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, color='blue')\n",
    "plt.plot(valid_losses, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "k = 12\n",
    "x_poly = make_poly(x[:8000], k)\n",
    "x_valid = make_poly(x[8000:], k)\n",
    "y_valid = y[8000:]\n",
    "lr = LR_valid(num_features=x_poly.shape[1])\n",
    "losses, valid_losses = lr.fit(x_poly, y[:8000], X_valid=x_valid, y_valid=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, color='blue')\n",
    "plt.plot(valid_losses, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "t_poly = make_poly(t, k)\n",
    "y_pred = lr.predict(t_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(t[:, 1], y_pred, color='red')\n",
    "plt.scatter(x[8000:, 1], y[8000:], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex8 Find train and valid losses for all polynomial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lrs = {'models': [], 'losses': [], 'train_loss_history': [], 'valid_loss_history': []}\n",
    "for k in range(1, 26):\n",
    "    x_poly_k = make_poly(x_small, k)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex9 Do the same thing as Ex8, but instead of using 15 samples use 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex10 Implement L2 and L1 regularizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W) = J_{old}(W) + alpha * (w_1^2 + ... + w_{p}^2)$$\n",
    "$$J_{old}(W)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LR_valid_L2:\n",
    "    def __init__(self, num_features=1, optimizer=utils.GD(0.05), alpha=0):\n",
    "        self.W = np.zeros((num_features, 1)) + 5\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = X @ self.W\n",
    "        return y\n",
    "    \n",
    "    def one_step_opt(self, X, y_true):\n",
    "        grad_reg = 2 * self.alpha * self.W\n",
    "        grad_reg[0, 0] = 0\n",
    "        reg_loss = np.sum(self.alpha * (self.W) ** 2)\n",
    "        grads = - X.T @ (y_true - X @ self.W) / X.shape[0] + grad_reg\n",
    "        self.W = self.optimizer.apply_grads(self.W, grads)\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss, grads, reg_loss\n",
    "    \n",
    "    def fit(self, X, y_true, X_valid=None, y_valid=None, grad_tol=0.0001, n_iters=10000):\n",
    "        grad_norm = np.inf\n",
    "        n_iter = 0\n",
    "        losses = []\n",
    "        valid_losses = []\n",
    "        reg_losses = []\n",
    "        while (grad_norm > grad_tol) and (n_iter < n_iters):\n",
    "            loss, grads, reg_loss = self.one_step_opt(X, y_true)\n",
    "            grad_norm = np.linalg.norm(grads)\n",
    "            n_iter += 1\n",
    "            losses.append(loss[0][0])\n",
    "            valid_loss = (y_valid - self.predict(X_valid)).T @ (y_valid - self.predict(X_valid)) / X_valid.shape[0]\n",
    "            valid_losses.append(valid_loss[0][0])\n",
    "            reg_losses.append(reg_loss)\n",
    "        return np.array(losses), np.array(valid_losses), np.array(reg_losses)\n",
    "    \n",
    "    def fit_closed_form(self, X, y_true, X_valid=None, y_valid=None):\n",
    "        self.W = np.linalg.inv(X.T @ X) @ X.T @ y_true\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 10))\n",
    "terminal_losses = []\n",
    "alphas = np.arange(0, 0.01, 0.001)\n",
    "Ws = []\n",
    "for alpha in alphas:\n",
    "\n",
    "    k = 17\n",
    "\n",
    "    x_poly = make_poly(x_small, k)\n",
    "    x_valid = make_poly(x[100:1100], k)\n",
    "    y_valid = y[100:1100]\n",
    "    lr = LR_valid_L2(num_features=x_poly.shape[1], alpha=alpha)\n",
    "    losses, valid_losses, reg_losses = lr.fit(x_poly, y_small, X_valid=x_valid, y_valid=y_valid)\n",
    "    terminal_losses.append(valid_losses[-1])\n",
    "    Ws.append(lr.W[2])\n",
    "    total_losses = losses + reg_losses\n",
    "    '''\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(losses, color='blue')\n",
    "    plt.plot(valid_losses, color='orange')\n",
    "    plt.plot(reg_losses, color='green')\n",
    "    plt.plot(total_losses, color='black')\n",
    "    plt.ylim((0, 0.005))\n",
    "    plt.title(f'deg={k}')\n",
    "\n",
    "    t_poly = make_poly(t, k)\n",
    "    y_pred = lr.predict(t_poly)\n",
    "    plt.subplot(2, 1, 2)\n",
    "\n",
    "    plt.plot(t[:, 1], y_pred, color='red')\n",
    "    plt.plot(t[:, 1], y_pred, color='red')\n",
    "    plt.scatter(x[8000:, 1], y[8000:], color='blue')\n",
    "    plt.scatter(x_small[:, 1], y_small, color='pink', s=50)\n",
    "    plt.ylim((-0.15, 0.15))\n",
    "    plt.title(f'deg={k}')\n",
    "    plt.show()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(alphas, Ws)\n",
    "#plt.ylim((0, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W) = J_{old}(W) + alpha * (|w_1| + ... + |w_p|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class LR_valid_L1:\n",
    "    def __init__(self, num_features=1, optimizer=utils.GD(0.05), alpha=0):\n",
    "        self.W = np.zeros((num_features, 1)) + 5\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y = X @ self.W\n",
    "        return y\n",
    "    \n",
    "    def one_step_opt(self, X, y_true):\n",
    "        grad_reg = self.alpha * np.sign(self.W)\n",
    "        grad_reg[0, 0] = 0\n",
    "        reg_loss = np.sum(self.alpha * np.abs(self.W))\n",
    "        grads = - X.T @ (y_true - X @ self.W) / X.shape[0] + grad_reg\n",
    "        self.W = self.optimizer.apply_grads(self.W, grads)\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss, grads, reg_loss\n",
    "    \n",
    "    def fit(self, X, y_true, X_valid=None, y_valid=None, grad_tol=0.0001, n_iters=10000):\n",
    "        grad_norm = np.inf\n",
    "        n_iter = 0\n",
    "        losses = []\n",
    "        valid_losses = []\n",
    "        reg_losses = []\n",
    "        while (grad_norm > grad_tol) and (n_iter < n_iters):\n",
    "            loss, grads, reg_loss = self.one_step_opt(X, y_true)\n",
    "            grad_norm = np.linalg.norm(grads)\n",
    "            n_iter += 1\n",
    "            losses.append(loss[0][0])\n",
    "            valid_loss = (y_valid - self.predict(X_valid)).T @ (y_valid - self.predict(X_valid)) / X_valid.shape[0]\n",
    "            valid_losses.append(valid_loss[0][0])\n",
    "            reg_losses.append(reg_loss)\n",
    "        return np.array(losses), np.array(valid_losses), np.array(reg_losses)\n",
    "    \n",
    "    def fit_closed_form(self, X, y_true, X_valid=None, y_valid=None):\n",
    "        self.W = np.linalg.inv(X.T @ X) @ X.T @ y_true\n",
    "        loss = (y_true - self.predict(X)).T @ (y_true - self.predict(X)) / X.shape[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "terminal_losses = []\n",
    "alphas = np.arange(0.01, 0.005, 0.001)\n",
    "Ws = []\n",
    "alpha = 0.003\n",
    "\n",
    "#for alpha in alphas:\n",
    "\n",
    "k = 17\n",
    "\n",
    "x_poly = make_poly(x_small, k)\n",
    "mu = np.mean(x_poly, axis=0)\n",
    "std = np.std(x_poly, axis=0)\n",
    "x_poly = (x_poly - mu) / std\n",
    "x_poly[:, 0] = 1\n",
    "\n",
    "x_valid = make_poly(x[100:1100], k)\n",
    "x_valid = (x_valid - mu) / std\n",
    "x_valid[:, 0] = 1\n",
    "y_valid = y[100:1100]\n",
    "lr = LR_valid_L1(num_features=x_poly.shape[1], alpha=alpha)\n",
    "losses, valid_losses, reg_losses = lr.fit(x_poly, y_small, X_valid=x_valid, y_valid=y_valid)\n",
    "terminal_losses.append(valid_losses[-1])\n",
    "Ws.append(lr.W[2])\n",
    "total_losses = losses + reg_losses\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(losses, color='blue')\n",
    "plt.plot(valid_losses, color='orange')\n",
    "plt.plot(reg_losses, color='green')\n",
    "plt.plot(total_losses, color='black')\n",
    "#plt.ylim((0, 0.005))\n",
    "plt.title(f'deg={k}')\n",
    "\n",
    "t_poly = make_poly(t, k)\n",
    "y_pred = lr.predict(t_poly)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.plot(t[:, 1], y_pred, color='red')\n",
    "plt.plot(t[:, 1], y_pred, color='red')\n",
    "plt.scatter(x[8000:, 1], y[8000:], color='blue')\n",
    "plt.scatter(x_small[:, 1], y_small, color='pink', s=50)\n",
    "plt.ylim((-0.15, 0.15))\n",
    "plt.title(f'deg={k}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "terminal_losses = []\n",
    "alphas = np.arange(0.009, 0.01, 0.0001)\n",
    "Ws = []\n",
    "\n",
    "k = 17\n",
    "\n",
    "x_poly = make_poly(x_small, k)\n",
    "x_valid = make_poly(x[100:1100], k)\n",
    "y_valid = y[100:1100]\n",
    "for alpha in alphas:\n",
    "\n",
    "    lr = LR_valid_L1(num_features=x_poly.shape[1], alpha=alpha)\n",
    "    losses, valid_losses, reg_losses = lr.fit(x_poly, y_small, X_valid=x_valid, y_valid=y_valid)\n",
    "    terminal_losses.append(valid_losses[-1])\n",
    "    Ws.append(lr.W[2])\n",
    "    total_losses = losses + reg_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fb84de0e",
   "language": "python",
   "display_name": "PyCharm (machine_learning)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}